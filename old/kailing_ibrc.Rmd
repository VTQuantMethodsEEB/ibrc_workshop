---
title: "Maintaining data and building pipelines in R"
author: "Kate E. Langwig, Alex T. Grimaudo, Macy J. Kailing - IBRC 2022"
output:
  ioslides_presentation: 
    incremental: true
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Field, lab, or software program to R

The first section will focus on best practices for organizing data **from the field, lab, or elsewhere** that will make the transition to wrangling it in R easiest.

## Learning objectives

### Improve data management through: {.smaller}

- Building pipelines
- Datasheet format
- File management
- Maintaining relational structure between data types
- Meta-data tracking

## Why pipelines?

Pipelines are ways of carefully recording and systematizing the steps you take to work with your data

### Ideally your project will depend on: {.smaller}

- Some data files: spreadsheets storing data (.csv files from Excel)
- Some scripts (more on this later)
- Something that tells you how these things go together: README file

## Getting started | *before beginning data collection*

### What will your data **look like** and how will you **analyze it**? {.smaller}

- Determine what your data points/observations/sampling units are (individual bat, acoustic detector, insect trap)
- Use these to link across data types
- Stay consistent by planning ahead 
    - spend a little time up front to save lots of time later
- Whenever possible, record in a way that reflects your data entry procedure

## For example...

To streamline the process between data collection and entry to spreadsheets and reduce transcription errors, we can ***structure datasheets to***:

- minimize extraneous information
- keep single observations to one row
- have distinct pages per sampling event or unit
- organize columns in the same order you will collect the data

## Managing data files and folders | *after you collect samples or receive results*

Proper file (electronic and hard copy) management will save you TONS of time, headaches, and **most importantly downstream errors**.

### ***Always...*** {.smaller}
- scan datasheets to save electronically and make copies
- retain unaltered original copies of datasheets or output files
- enter data from scanned sheets 
- develop a quality control procedure

## ---
### ***Consider...*** {.smaller}
- organizing as if you will continuously build the dataset and anticipate you will need to go back
- "*if someone continued this project, is my data pipeline in a place I could hand it over seamlessly?*"

## An example with acoustic data | multiple sites; multiple detectors; thousands of calls

This can quickly become overwhelming with an abundance of folders and files; each containing many dates and program outputs

## Tips for organizing bulky data streams
- Organize by detector and give each a unique name
- Add prefix to all call files with unique name (bulk rename utility for PCs/select multiple and rename on Mac)
- Store call files in folders by each **detector**
- Minimize transfer of files by keeping folder structure simple (don't create unnecessary sub-folders)
- Store together the **output files** (from analysis software) you will work with directly 
- Store raw files separately and safely

## Relational structure | *collating multiple streams of data*

Set yourself up to easily combine different types of data

- Each observation (or row) within any dataframe needs a unique identifier (i.e., sample ID)
- Maintain the same unique identifiers across **ALL YOUR DATA FILES** until the end of time :)
- If observations differ in scale, create unique identifier at the lowest possible level (sampling unit instead of individual observation) 
    - Easier to move up than down

## *Metadata* and data dictionaries | *preserving tangential information*

How do you store data that is important but not specific to a single 'observation'?

- Net effort, environmental conditions during sampling event, site characteristics, etc
- Create a separate metadata file that includes the same variables as observational data (e.g., site, date, species)

## Metadata and *data dictionaries* | *preserving tangential information*

How would someone stepping into the project understand the data you collect?

- We often collect data that is not intuitive to non-bat biologists (wing score, RFA, reproductive condition, etc)

- ***Make a data dictionary***: 
    - A table of definitions in an Excel Workbook (or other program)
    - Each variable from the datasheet is described thoroughly, including precisely what it measures

## Record keeping | *long-term pipeline success*

### You will inevitably forget your procedure.
- Write a protocol for how your files are organized/linked together
- Track any modifications using your README file or other notebook. 
- ***Advantages***:
    - Allows more flexibility and time saving if you need someone to help
    - Provides an easy reference for yourself
    - Reminds you how you set up your pipeline if you need to be away from the project for any time

## Now, data is collected, entered to spreadsheets, and filed appropriately!

### Time to work in R..

